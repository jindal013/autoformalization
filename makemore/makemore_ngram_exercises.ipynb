{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96561d4a-032c-404c-8798-5392127acf26",
   "metadata": {},
   "source": [
    "# Trigram Language Model\n",
    "1) Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "2) Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "3) Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "4) We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n",
    "5) Look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96aa5e68-5d22-4159-8e81-0431b902c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6434418-2d5b-4aac-9762-c6accd14bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "itos = {i+1: s for i, s in enumerate(chars)}\n",
    "itos[0] = '.'\n",
    "stoi = {s: i for i, s in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11cc0ff8-bb7a-4078-93e8-a2b5678d5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab_size = len(itos.items())\n",
    "g = torch.Generator().manual_seed(1)\n",
    "\n",
    "N = torch.zeros((vocab_size * vocab_size, vocab_size), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8f30fc20-37e8-49cb-a83c-6ae4815cf50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182472, 2]) torch.Size([182472])\n",
      "torch.Size([22856, 2]) torch.Size([22856])\n",
      "torch.Size([22818, 2]) torch.Size([22818])\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    xs, ys = [], []\n",
    "    block_size = 2 # trigram model\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "    \n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            xs.append(context)\n",
    "            ys.append(ix)\n",
    "            # print('-'.join(str(i) for i in context), '--->', ix)\n",
    "                    \n",
    "            index = 27*context[0] + context[1]\n",
    "            N[index, ix] += 1\n",
    "            \n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(xs)\n",
    "    Y = torch.tensor(ys)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(10)\n",
    "random.shuffle(words)\n",
    "\n",
    "l = len(words)\n",
    "n1 = int(0.8*l)\n",
    "n2 = int(0.9*l)\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "47599fcd-b256-4f30-ba06-94d988cde68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e50816fc-89fd-4462-96ff-1a2d4cf512d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anuee\n",
      "nvtps\n",
      "marian\n",
      "dante\n",
      "na\n",
      "silayley\n",
      "kemah\n",
      "lucin\n",
      "epiccaleen\n",
      "dmzi\n",
      "kence\n",
      "jordon\n",
      "kalla\n",
      "miqrqyjaya\n",
      "vihia\n",
      "acen\n",
      "kaitharcephelia\n",
      "son\n",
      "chieliyos\n",
      "gan\n"
     ]
    }
   ],
   "source": [
    "# sampling from the model\n",
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "\n",
    "    while True:\n",
    "        ix = 27*context[0] + context[1] # convert to corresponding row\n",
    "        p = P[ix]\n",
    "\n",
    "        iout = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if iout == 0:\n",
    "            break\n",
    "        out.append(itos[iout])\n",
    "        context = context[1:] + [iout]\n",
    "\n",
    "    print(''.join(out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9521644-7caa-46d6-b79d-90c6a8f78aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-504653.)\n",
      "nll=tensor(504653.)\n",
      "2.2119739055633545\n"
     ]
    }
   ],
   "source": [
    "# evaluating the loss\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        index = 27*context[0] + context[1]\n",
    "        iout = stoi[ch]\n",
    "        prob = P[index, iout]\n",
    "        ll = prob.log()\n",
    "        log_likelihood += ll\n",
    "        n += 1\n",
    "        context = context[1:] + [iout]\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e97ae3-8021-4a30-aa60-78cabd3049b4",
   "metadata": {},
   "source": [
    "### Trigram model with NN\n",
    "dataset --> init model --> gradient descent (forward pass, calculate loss, backward, update) --> sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "96f9ce36-c531-4bb7-a892-9d740ea2dba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182556]) torch.Size([182556])\n",
      "torch.Size([22774]) torch.Size([22774])\n",
      "torch.Size([22816]) torch.Size([22816])\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    xs, ys = [], []\n",
    "    block_size = 2 # trigram model\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "    \n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            index = 27*context[0] + context[1]            \n",
    "\n",
    "            xs.append(index)\n",
    "            ys.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(xs)\n",
    "    Y = torch.tensor(ys)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(10)\n",
    "random.shuffle(words)\n",
    "\n",
    "l = len(words)\n",
    "n1 = int(0.8*l)\n",
    "n2 = int(0.9*l)\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "num = Xtr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "df6f9d54-89fb-4b25-881d-c7bc123dba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "g = torch.Generator().manual_seed(10)\n",
    "\n",
    "# initialize the network\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "26193d42-b3f2-4cf1-bc45-d7f79dab4c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.228956460952759\n",
      "2.2286083698272705\n",
      "2.228269100189209\n",
      "2.227938652038574\n",
      "2.227616548538208\n",
      "2.2273025512695312\n",
      "2.226996660232544\n",
      "2.226698160171509\n",
      "2.2264068126678467\n",
      "2.2261226177215576\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    # forward pass\n",
    "    # xenc = F.one_hot(Xtr, num_classes=729).float()\n",
    "    logits = W[Xtr]\n",
    "    loss = F.cross_entropy(logits, Ytr) + 0.01*(W**2).mean()\n",
    "    # counts = logits.exp()\n",
    "    # probs = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = -probs[torch.arange(num), Ytr].log().mean() + 0.01*(W**2).mean()\n",
    "    if i % 10 == 9:\n",
    "        print(loss.item()) \n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -100*W.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ff0aebbb-d1fd-4778-a845-670d63867176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.24782657623291"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss on dev set - for hyperparameter tuning (regularization strength, learning rate)\n",
    "\n",
    "# xenc = F.one_hot(Xdev, num_classes=729).float()\n",
    "logits = W[Xdev] # xenc @ W\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "# counts = logits.exp()\n",
    "# probs = counts / counts.sum(1, keepdim=True)\n",
    "# loss = -probs[torch.arange(Xdev.shape[0]), Ydev].log().mean()\n",
    "\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a019d0dc-1798-4e68-9cfe-284970691061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.250016450881958"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss on test set - eval only once\n",
    "\n",
    "# xenc = F.one_hot(Xte, num_classes=729).float()\n",
    "logits = W[Xte] # xenc @ W\n",
    "loss = F.cross_entropy(logits, Yte)\n",
    "# counts = logits.exp()\n",
    "# probs = counts / counts.sum(1, keepdim=True)\n",
    "# loss = -probs[torch.arange(Xte.shape[0]), Yte].log().mean()\n",
    "\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7121f07-0119-407a-a557-135a90f1f65e",
   "metadata": {},
   "source": [
    "Cross_entropy represents a more optimized function from torch library that is able to speed up the computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "af750b28-5852-4fff-addb-2de7df17f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anuguelvtps\n",
      "marian\n",
      "dante\n",
      "na\n",
      "silayley\n",
      "kemah\n",
      "luman\n",
      "epjccuoden\n",
      "dazi\n",
      "kence\n",
      "jordon\n",
      "kalla\n",
      "miqrqyjaya\n",
      "vihia\n",
      "acen\n",
      "kaitharcephelia\n",
      "son\n",
      "chieliyos\n",
      "gan\n",
      "abren\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    context = [0] * block_size\n",
    "    out = []\n",
    "\n",
    "    while True:\n",
    "        index = 27*context[0] + context[1]\n",
    "        enc = F.one_hot(torch.tensor([index]), num_classes=729).float()\n",
    "        logits = enc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "        out.append(itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "        \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9b4b5-d16c-404f-8339-9a37fa227afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
